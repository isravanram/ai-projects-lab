{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOw8yMd1VlnD"
      },
      "source": [
        "# Data Preprocessing Template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvUGC8QQV6bV"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfFEXZC0WS-V"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhYaZ-ENV_c5"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqHTg9bxWT_u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9299e17b-c6d5-4764-9a8e-6f4a92ef2786"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "\n",
        "file_path = '/content/drive/My Drive/AI-projects-lab/ml-datasets/preprocessing_sample.csv'\n",
        "dataset = pd.read_csv(file_path)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "<class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(dataset))\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values\n",
        "\n",
        "print('\\nX value')\n",
        "print(X)\n",
        "print('\\nY value:')\n",
        "print(y)\n",
        "\n",
        "print('\\nNull records count:\\n{0}'.format(dataset.isna().sum()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUmbjo4Z8maR",
        "outputId": "a7209678-3538-4a4c-f101-6b77a0136c4c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "\n",
            "X value\n",
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n",
            "\n",
            "Y value:\n",
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n",
            "\n",
            "Null records count:\n",
            "Country      0\n",
            "Age          1\n",
            "Salary       1\n",
            "Purchased    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Taking care of missing data\n",
        "\n",
        "**SimpleImputer**:\n",
        "\n",
        "SimpleImputer is a class in scikit-learn that provides basic strategies for imputing (i.e., filling in) missing values in a dataset.\n",
        "\n",
        "1) missing_values=np.nan:\n",
        "\n",
        "This parameter specifies what value the imputer should consider as \"missing.\"\n",
        "np.nan (from the numpy library) represents missing or undefined values in a dataset.\n",
        "By setting missing_values=np.nan, you are telling the imputer to look for NaN values in your dataset to impute.\n",
        "\n",
        "2) strategy='mean':\n",
        "\n",
        "This parameter specifies the imputation strategy, i.e., how the missing values should be filled.\n",
        "strategy='mean' means that the imputer will calculate the mean of the non-missing values in each column and replace any missing values (NaN) with this mean.\n",
        "Other strategies you could use include:\n",
        "'median': Replace missing values with the median of the column.\n",
        "'most_frequent': Replace missing values with the most frequent value in the column.\n",
        "'constant': Replace missing values with a constant value (specified by the fill_value parameter)."
      ],
      "metadata": {
        "id": "2UhnSgACGT7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Error code Cannot use mean strategy with non-numeric data: could not convert string to float: 'France'\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imputer.fit(X)\n",
        "\n",
        "# Only apply transformations to these columns\n",
        "X = imputer.transform(X)\n",
        "\n",
        "# Apply transformations\n",
        "print(X)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "F-4SLdtGMMrF",
        "outputId": "d71eb66c-8817-4ef0-8aa3-b65e0f421c27"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot use mean strategy with non-numeric data:\ncould not convert string to float: 'France'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-c8f328c21c1a>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimputer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mimputer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Only apply transformations to these columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 )\n\u001b[1;32m   1151\u001b[0m             ):\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/impute/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \"\"\"\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_fit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;31m# default fill_value is 0 for numerical input and \"missing_value\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/impute/_base.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, X, in_fit)\u001b[0m\n\u001b[1;32m    328\u001b[0m                     )\n\u001b[1;32m    329\u001b[0m                 )\n\u001b[0;32m--> 330\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mnew_ve\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot use mean strategy with non-numeric data:\ncould not convert string to float: 'France'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imputer.fit(X[:, 1:3])\n",
        "\n",
        "# Only apply transformations to these columns\n",
        "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
        "\n",
        "# Apply transformations\n",
        "print(X)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6ul52QDGSAv",
        "outputId": "bb38401a-689b-41d8-cc50-e1a5acc3108b"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding categorical data\n",
        "\n",
        "**ColumnTransformer:**\n",
        "The ColumnTransformer is a class in scikit-learn that allows you to apply different transformations to different columns of your dataset.\n",
        "It takes a list of transformers and applies them to specified columns.\n",
        "\n",
        "1) transformers=[...]:\n",
        "\n",
        "This parameter specifies a list of transformations to be applied. Each item in the list is a tuple containing three elements:\n",
        "A name for the transformer: 'encoder' in this case.\n",
        "The transformer object: OneHotEncoder() here.\n",
        "The column(s) to apply the transformer to: [0], which refers to the first column of your dataset.\n",
        "\n",
        "2) OneHotEncoder():\n",
        "\n",
        "This is a transformer that converts categorical values into a format that can be provided to ML algorithms to do a better job in prediction.\n",
        "One-hot encoding turns a categorical column with n unique values into n binary columns, where each column corresponds to one of the unique values in the original column.\n",
        "For example, if the first column contains categories like ['red', 'blue', 'green'], OneHotEncoder will create three new columns, one for each category.\n",
        "\n",
        "3) remainder='passthrough':\n",
        "\n",
        "This parameter tells ColumnTransformer what to do with the columns that are not explicitly transformed by the transformers list.\n",
        "'passthrough' means that all other columns should be left unchanged and passed through in the final output.\n",
        "Other options include 'drop' to drop the other columns or applying another transformer to them.\n",
        "\n",
        "4) X = np.array(ct.fit_transform(X)):\n",
        "\n",
        "fit_transform(X) applies the transformation(s) defined in the ColumnTransformer to the data X.\n",
        "This line transforms the specified columns in X according to the provided transformers (in this case, one-hot encoding the first column) and returns the transformed array.\n",
        "\n",
        "The result is then converted to a NumPy array using np.array()."
      ],
      "metadata": {
        "id": "ZVoKIVqsKB2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding dependent variables\n",
        "\n",
        "# In this case Country column is the independent categorical value\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "ct = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('encoder', OneHotEncoder(), [0])\n",
        "        ],\n",
        "     remainder='passthrough'\n",
        "     )\n",
        "transformed_X = ct.fit_transform(X)\n",
        "print('Transformed X:\\n{0}'.format(transformed_X))\n",
        "X = np.array(transformed_X)\n",
        "print('\\nEncoded X:\\n{0}'.format(X))\n",
        "\n",
        "\n",
        "# Encoding independent variables\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsP4_S-zKcBk",
        "outputId": "25c66a0d-e512-4c0d-a5fe-5a1da3d518ac"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformed X:\n",
            "[[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n",
            "\n",
            "Encoded X:\n",
            "[[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3abSxRqvWEIB"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm48sif-WWsh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "558efa2a-6050-4a83-f139-3868177beb14"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "print('Record count for each split :\\nX_train:{x_train}, X_test:{x_test}, Y_train:{y_train}, Y_test:{y_test}'.format(x_train= len(X_train),x_test = len(X_test),y_train=len(y_train),y_test = len(y_test)))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Record count for each split :\n",
            "X_train:8, X_test:2, Y_train:8, Y_test:2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Scaling"
      ],
      "metadata": {
        "id": "-ZeJ6CMoQWNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.preprocessing import StandardScaler:\n",
        "\n",
        "This imports the StandardScaler class from the sklearn.preprocessing module. StandardScaler is a tool used to standardize the features in your dataset.\n",
        "sc = StandardScaler():\n",
        "\n",
        "This line creates an instance of the StandardScaler class, named sc.\n",
        "StandardScaler standardizes features by removing the mean and scaling to unit variance.\n",
        "X_train = sc.fit_transform(X_train):\n",
        "\n",
        "**fit_transform(X_train):**\n",
        "The fit_transform method does two things:\n",
        "\n",
        "**fit(X_train):** It calculates the mean and standard deviation of each feature in the X_train dataset. This is the \"fitting\" process.\n",
        "\n",
        "**transform(X_train):** It applies the transformation to the X_train dataset, using the mean and standard deviation calculated during the fit step.\n",
        "After this, all features in X_train will have a mean of 0 and a standard deviation of 1.\n",
        "This standardization process ensures that each feature contributes equally to the model, which is particularly important for algorithms that rely on distance calculations (e.g., k-nearest neighbors, SVM) or gradient descent optimization (e.g., linear regression, logistic regression, neural networks).\n",
        "X_test = sc.transform(X_test):\n",
        "\n",
        "transform(X_test):\n",
        "This method applies the same transformation to the X_test dataset that was applied to X_train, using the mean and standard deviation calculated from X_train.\n",
        "Important: fit_transform is only applied to the training data (X_train) to avoid data leakage. We do not \"fit\" the scaler on X_test because the test data should be completely unseen by the model during training. Instead, we simply use the parameters learned from the training data to transform the test data.\n",
        "\n",
        "\n",
        "\n",
        "**Impact of Data Leakage:**\n",
        "\n",
        "**Overfitting:**\n",
        "\n",
        "The test data has influenced the mean and standard deviation calculations. The model might now perform exceptionally well on X_test during validation, giving you an overly optimistic view of its performance.\n",
        "\n",
        "\n",
        "**Poor Generalization:**\n",
        "\n",
        " Since the test data \"leaked\" into the training process, the model might not generalize well to truly unseen data (like a new test set or data in production).\n",
        "\n",
        "\n",
        "**Biased Model Evaluation:**\n",
        "\n",
        "The evaluation metrics (e.g., accuracy, RMSE) calculated on the test set might be misleading because the model had indirect access to test data characteristics during training."
      ],
      "metadata": {
        "id": "k4wUK_XaVxa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "print(X_train)\n",
        "print(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "997Pxv-N94MU",
        "outputId": "d8bec210-d19a-40fa-dbd4-c7520376851d"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.          2.64575131 -0.77459667  0.26306757  0.12381479]\n",
            " [ 1.         -0.37796447 -0.77459667 -0.25350148  0.46175632]\n",
            " [-1.         -0.37796447  1.29099445 -1.97539832 -1.53093341]\n",
            " [-1.         -0.37796447  1.29099445  0.05261351 -1.11141978]\n",
            " [ 1.         -0.37796447 -0.77459667  1.64058505  1.7202972 ]\n",
            " [-1.         -0.37796447  1.29099445 -0.0813118  -0.16751412]\n",
            " [ 1.         -0.37796447 -0.77459667  0.95182631  0.98614835]\n",
            " [ 1.         -0.37796447 -0.77459667 -0.59788085 -0.48214934]]\n",
            "[[-1.          2.64575131 -0.77459667 -1.45882927 -0.90166297]\n",
            " [-1.          2.64575131 -0.77459667  1.98496442  2.13981082]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example:\n",
        "\n",
        "X_train = [\n",
        "    \n",
        "    [3, 2000, 400000],  # 3 bedrooms, 2000 sqft, $400,000\n",
        "    [4, 2500, 500000],  # 4 bedrooms, 2500 sqft, $500,000\n",
        "    [2, 1500, 300000]   # 2 bedrooms, 1500 sqft, $300,000\n",
        "]\n",
        "\n",
        "\n",
        "Mean: [3, 2000, 400000]\n",
        "\n",
        "\n",
        "Standard Deviation: [1, 500, 100000]\n",
        "\n",
        "\n",
        "\n",
        "X_train_scaled = [\n",
        "\n",
        "    [0, 0, 0],   # (3-3)/1, (2000-2000)/500, (400000-400000)/100000\n",
        "    [1, 1, 1],   # (4-3)/1, (2500-2000)/500, (500000-400000)/100000\n",
        "    [-1, -1, -1] # (2-3)/1, (1500-2000)/500, (300000-400000)/100000\n",
        "]"
      ],
      "metadata": {
        "id": "511sbtezVkpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4EZYW_PpVjWw"
      }
    }
  ]
}